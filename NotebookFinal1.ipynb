{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Reshape, Activation, Dropout\n",
    "from keras.layers import Concatenate, Add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "os.chdir('code')\n",
    "from utils import tf_haversine,get_clusters\n",
    "from data import load_data\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benmosbahmohamed/KerasSolution/DeepLeraning_PredictionTaxi/code/utils.py:83: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  clusters = clusters.as_matrix()\n"
     ]
    }
   ],
   "source": [
    "clusters = get_clusters(data.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation, Dropout,Input\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers import Concatenate,Add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from utils import tf_haversine\n",
    "from data import load_data\n",
    "from utils import get_clusters\n",
    "from keras.layers import concatenate\n",
    "\n",
    "\n",
    "\n",
    "def start_new_session():\n",
    "    \"\"\"\n",
    "    Starts a new Tensorflow session.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the session only uses the GPU memory that it actually needs\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    session = tf.Session(config=config, graph=tf.get_default_graph())\n",
    "    K.tensorflow_backend.set_session(session)\n",
    "\n",
    "\n",
    "def first_last_k(coords):\n",
    "    \"\"\"\n",
    "    Returns a list with the first k and last k GPS coordinates from the given trip.\n",
    "    The returned list contains 4k values (latitudes and longitudes for 2k points).\n",
    "    \"\"\"\n",
    "    k = 5\n",
    "    partial = [coords[0] for i in range(2*k)]\n",
    "    num_coords = len(coords)\n",
    "    if num_coords < 2*k:\n",
    "        partial[-num_coords:] = coords\n",
    "    else:\n",
    "        partial[:k] = coords[:k]\n",
    "        partial[-k:] = coords[-k:]\n",
    "    partial = np.row_stack(partial)\n",
    "    return np.array(partial).flatten()\n",
    "\n",
    "\n",
    "def process_features(df):\n",
    "    \"\"\"\n",
    "    Process the features required by our model from the given dataframe.\n",
    "    Return the features in a list so that they can be merged in our model's input layer.\n",
    "    \"\"\"\n",
    "    # Fetch the first and last GPS coordinates\n",
    "    coords = np.row_stack(df['POLYLINE'].apply(first_last_k))\n",
    "    # Standardize latitudes (odd columns) and longitudes (even columns)\n",
    "    latitudes = coords[:,::2]\n",
    "    coords[:,::2] = scale(latitudes)\n",
    "    longitudes = coords[:,1::2]\n",
    "    coords[:,1::2] = scale(longitudes)\n",
    "    \n",
    "    return [\n",
    "        df['QUARTER_HOUR'].as_matrix(),\n",
    "        df['DAY_OF_WEEK'].as_matrix(),\n",
    "        df['WEEK_OF_YEAR'].as_matrix(),\n",
    "        df['ORIGIN_CALL_ENCODED'].as_matrix(),\n",
    "        df['TAXI_ID_ENCODED'].as_matrix(),\n",
    "        df['ORIGIN_STAND_ENCODED'].as_matrix(),\n",
    "        coords,\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_model(metadata, clusters):\n",
    "    \"\"\"\n",
    "    Creates all the layers for our neural network model.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Arbitrary dimension for all embeddings\n",
    "    embedding_dim = 10\n",
    "\n",
    "    # Quarter hour of the day embedding\n",
    "    input_quarter_hour = Input(shape=(1,))\n",
    "    embed_quarter_hour = Embedding(metadata['n_quarter_hours'], embedding_dim, input_length=1)(input_quarter_hour)\n",
    "    embed_quarter_hour = Reshape((embedding_dim,))(embed_quarter_hour)\n",
    "\n",
    "    # Day of the week embedding\n",
    "    input_day_of_week = Input(shape=(1,))\n",
    "    embed_day_of_week = Embedding(metadata['n_days_per_week'], embedding_dim, input_length=1)(input_day_of_week)\n",
    "    embed_day_of_week = Reshape((embedding_dim,))(embed_day_of_week)\n",
    "\n",
    "    # Week of the year embedding\n",
    "    input_week_of_year = Input(shape=(1,))\n",
    "    embed_week_of_year = Embedding(metadata['n_weeks_per_year'], embedding_dim, input_length=1)(input_week_of_year)\n",
    "    embed_week_of_year = Reshape((embedding_dim,))(embed_week_of_year)\n",
    "\n",
    "    # Client ID embedding\n",
    "    input_client_ids = Input(shape=(1,))\n",
    "    embed_client_ids = Embedding(metadata['n_client_ids'], embedding_dim, input_length=1)(input_client_ids)\n",
    "    embed_client_ids = Reshape((embedding_dim,))(embed_client_ids)\n",
    "\n",
    "\n",
    "    # Taxi ID embedding\n",
    "    input_taxi_ids = Input(shape=(1,))\n",
    "    embed_taxi_ids = Embedding(metadata['n_taxi_ids'], embedding_dim, input_length=1)(input_taxi_ids)\n",
    "    embed_taxi_ids = Reshape((embedding_dim,))(embed_taxi_ids)\n",
    "\n",
    "\n",
    "    # Taxi stand ID embedding\n",
    "    input_stand_ids = Input(shape=(1,))\n",
    "    embed_stand_ids = Embedding(metadata['n_stand_ids'], embedding_dim, input_length=1)(input_stand_ids)\n",
    "    embed_stand_ids = Reshape((embedding_dim,))(embed_stand_ids)\n",
    "    \n",
    "    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n",
    "\n",
    "    coords_in = Input(shape=(20,))\n",
    "    \n",
    "    #model = Sequential()\n",
    "    \n",
    "    concatenated = concatenate([\n",
    "                embed_quarter_hour,\n",
    "                embed_day_of_week,\n",
    "                embed_week_of_year,\n",
    "                embed_client_ids,\n",
    "                embed_taxi_ids,\n",
    "                embed_stand_ids,\n",
    "                coords_in\n",
    "            ])\n",
    "    \n",
    "    out = Dense(500, activation='relu')(concatenated)\n",
    "    \n",
    "    out = Dense(len(clusters),activation='softmax',name='output_layer')(out)\n",
    "    \n",
    "    cast_clusters = K.cast_to_floatx(clusters)\n",
    "    def destination(probabilities):\n",
    "        return tf.matmul(probabilities, cast_clusters)\n",
    "    \n",
    "    out = Activation(destination)(out)\n",
    "    \n",
    "    model = Model([\n",
    "                input_quarter_hour,\n",
    "                input_day_of_week,\n",
    "                input_week_of_year,\n",
    "                input_client_ids,\n",
    "                input_taxi_ids,\n",
    "                input_stand_ids,\n",
    "                coords_in\n",
    "            ],out)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = SGD(lr=0.01, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n",
    "    model.compile(loss=tf_haversine, optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def full_train(data,clusters,n_epochs=100, batch_size=200, save_prefix=None):\n",
    "    \"\"\"\n",
    "    Runs the complete training process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load initial data\n",
    "    print(\"Loading data...\")\n",
    "    #data = load_data()\n",
    "    data = data\n",
    "    \n",
    "    # Estimate the GPS clusters\n",
    "    print(\"Estimating clusters...\")\n",
    "    #clusters = get_clusters(data.train_labels)\n",
    "    clusters = clusters\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = []\n",
    "    if save_prefix is not None:\n",
    "        # Save the model's intermediary weights to disk after each epoch\n",
    "        file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n",
    "        callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    start_new_session()\n",
    "    model = create_model(data.metadata, clusters)\n",
    "    \n",
    "    # Run the training\n",
    "    print(\"Start training...\")\n",
    "    history = model.fit(\n",
    "        process_features(data.train), data.train_labels,\n",
    "        nb_epoch=n_epochs, batch_size=batch_size,\n",
    "        validation_data=(process_features(data.validation), data.validation_labels),\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    if save_prefix is not None:\n",
    "        # Save the training history to disk\n",
    "        file_path = 'cache/%s-history.pickle' % save_prefix\n",
    "        with open(file_path, 'wb') as handle:\n",
    "            pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Je regarde à quel point il est bon\n",
    "#Je load les weights du modèle pas la peine de réentraîner hehe\n",
    "start_new_session()\n",
    "model = create_model(data.metadata, clusters)\n",
    "#c'est là que je load les weights\n",
    "os.chdir('cache')\n",
    "model.load_weights('TrainedOn10_2_5-015-1.4894.hdf5')\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benmosbahmohamed/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:64: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/benmosbahmohamed/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/benmosbahmohamed/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/benmosbahmohamed/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/benmosbahmohamed/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:68: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/benmosbahmohamed/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "#Les prédictions sur l'ensemble de test de la compet (tout petit)\n",
    "test_predictions = model.predict(process_features(data.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5057129510896556"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('code')\n",
    "from utils import np_haversine\n",
    "os.chdir('..')\n",
    "np_haversine(test_predictions, data.test_labels).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En s'entraînant que sur le dixième des données, on obtient une loss de 1.46 (km) sur le truc de test. C'est mieux que sur le blog... Soit j'ai mal copié un truc qui fait que ça marche mieux hahah, soit keras 2.x et meilleur que 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
