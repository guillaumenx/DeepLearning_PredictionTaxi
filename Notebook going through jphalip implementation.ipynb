{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Reshape, Activation, Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "os.chdir('code')\n",
    "from utils import tf_haversine,get_clusters\n",
    "from data import load_data\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "all_coords = np.concatenate(data.train['POLYLINE_FULL'].as_matrix())\n",
    "clusters = get_clusters(all_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_new_session():\n",
    "    \"\"\"\n",
    "    Starts a new Tensorflow session.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the session only uses the GPU memory that it actually needs\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    session = tf.Session(config=config, graph=tf.get_default_graph())\n",
    "    K.tensorflow_backend.set_session(session)\n",
    "\n",
    "\n",
    "def first_last_k(coords):\n",
    "    \"\"\"\n",
    "    Returns a list with the first k and last k GPS coordinates from the given trip.\n",
    "    The returned list contains 4k values (latitudes and longitudes for 2k points).\n",
    "    \"\"\"\n",
    "    k = 5\n",
    "    partial = [coords[0] for i in range(2*k)]\n",
    "    num_coords = len(coords)\n",
    "    if num_coords < 2*k:\n",
    "        partial[-num_coords:] = coords\n",
    "    else:\n",
    "        partial[:k] = coords[:k]\n",
    "        partial[-k:] = coords[-k:]\n",
    "    partial = np.row_stack(partial)\n",
    "    return np.array(partial).flatten()\n",
    "\n",
    "\n",
    "def process_features(df):\n",
    "    \"\"\"\n",
    "    Process the features required by our model from the given dataframe.\n",
    "    Return the features in a list so that they can be merged in our model's input layer.\n",
    "    \"\"\"\n",
    "    # Fetch the first and last GPS coordinates\n",
    "    coords = np.row_stack(df['POLYLINE'].apply(first_last_k))\n",
    "    # Standardize latitudes (odd columns) and longitudes (even columns)\n",
    "    latitudes = coords[:,::2]\n",
    "    coords[:,::2] = scale(latitudes)\n",
    "    longitudes = coords[:,1::2]\n",
    "    coords[:,1::2] = scale(longitudes)\n",
    "    \n",
    "    return [\n",
    "        df['QUARTER_HOUR'].as_matrix(),\n",
    "        df['DAY_OF_WEEK'].as_matrix(),\n",
    "        df['WEEK_OF_YEAR'].as_matrix(),\n",
    "        df['ORIGIN_CALL_ENCODED'].as_matrix(),\n",
    "        df['TAXI_ID_ENCODED'].as_matrix(),\n",
    "        df['ORIGIN_STAND_ENCODED'].as_matrix(),\n",
    "        coords,\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_model(metadata, clusters):\n",
    "    \"\"\"\n",
    "    Creates all the layers for our neural network model.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Arbitrary dimension for all embeddings\n",
    "    embedding_dim = 10\n",
    "\n",
    "    # Quarter hour of the day embedding\n",
    "    embed_quarter_hour = Sequential()\n",
    "    embed_quarter_hour.add(Embedding(metadata['n_quarter_hours'], embedding_dim, input_length=1))\n",
    "    embed_quarter_hour.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Day of the week embedding\n",
    "    embed_day_of_week = Sequential()\n",
    "    embed_day_of_week.add(Embedding(metadata['n_days_per_week'], embedding_dim, input_length=1))\n",
    "    embed_day_of_week.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Week of the year embedding\n",
    "    embed_week_of_year = Sequential()\n",
    "    embed_week_of_year.add(Embedding(metadata['n_weeks_per_year'], embedding_dim, input_length=1))\n",
    "    embed_week_of_year.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Client ID embedding\n",
    "    embed_client_ids = Sequential()\n",
    "    embed_client_ids.add(Embedding(metadata['n_client_ids'], embedding_dim, input_length=1))\n",
    "    embed_client_ids.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Taxi ID embedding\n",
    "    embed_taxi_ids = Sequential()\n",
    "    embed_taxi_ids.add(Embedding(metadata['n_taxi_ids'], embedding_dim, input_length=1))\n",
    "    embed_taxi_ids.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Taxi stand ID embedding\n",
    "    embed_stand_ids = Sequential()\n",
    "    embed_stand_ids.add(Embedding(metadata['n_stand_ids'], embedding_dim, input_length=1))\n",
    "    embed_stand_ids.add(Reshape((embedding_dim,)))\n",
    "    \n",
    "    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n",
    "    coords = Sequential()\n",
    "    coords.add(Dense(1, input_dim=20, init='normal'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Concatenate([\n",
    "                embed_quarter_hour,\n",
    "                embed_day_of_week,\n",
    "                embed_week_of_year,\n",
    "                embed_client_ids,\n",
    "                embed_taxi_ids,\n",
    "                embed_stand_ids,\n",
    "                coords\n",
    "            ]))\n",
    "\n",
    "    # Simple hidden layer\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Determine cluster probabilities using softmax\n",
    "    model.add(Dense(len(clusters)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Final activation layer: calculate the destination as the weighted mean of cluster coordinates\n",
    "    cast_clusters = K.cast_to_floatx(clusters)\n",
    "    def destination(probabilities):\n",
    "        return tf.matmul(probabilities, cast_clusters)\n",
    "    model.add(Activation(destination))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = SGD(lr=0.01, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n",
    "    model.compile(loss=tf_haversine, optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def full_train(n_epochs=100, batch_size=200, save_prefix=None):\n",
    "    \"\"\"\n",
    "    Runs the complete training process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load initial data\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()\n",
    "\n",
    "    # Estimate the GPS clusters\n",
    "    print(\"Estimating clusters...\")\n",
    "    #J'ai fait ça mais à la base ils avaient mis ça :\n",
    "    #clusters = get_clusters(data.train_labels)\n",
    "    clusters = get_clusters(all_coords)\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = []\n",
    "    if save_prefix is not None:\n",
    "        # Save the model's intermediary weights to disk after each epoch\n",
    "        file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n",
    "        callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    start_new_session()\n",
    "    model = create_model(data.metadata, clusters)\n",
    "    \n",
    "    # Run the training\n",
    "    print(\"Start training...\")\n",
    "    history = model.fit(\n",
    "        process_features(data.train), data.train_labels,\n",
    "        nb_epoch=n_epochs, batch_size=batch_size,\n",
    "        validation_data=(process_features(data.validation), data.validation_labels),\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    if save_prefix is not None:\n",
    "        # Save the training history to disk\n",
    "        file_path = 'cache/%s-history.pickle' % save_prefix\n",
    "        with open(file_path, 'wb') as handle:\n",
    "            pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Estimating clusters...\n",
      "Creating model...\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:111: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, input_dim=20, kernel_initializer=\"normal\")`\n",
      "C:\\Users\\Mohamed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:178: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-0e122f66d43c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfull_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-b48b2f2b6f44>\u001b[0m in \u001b[0;36mfull_train\u001b[1;34m(n_epochs, batch_size, save_prefix)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_prefix\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# to match the value shapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;31m# since `Sequential` depends on `Model`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m                 \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
