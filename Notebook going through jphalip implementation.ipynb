{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import calendar \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns #Pour les graphiques\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import  MultipleLocator, FormatStrFormatter\n",
    "from scipy.interpolate import spline\n",
    "from IPython.core.display import display_html\n",
    "from keras.models import load_model\n",
    "import os \n",
    "os.chdir('code')\n",
    "from utils import np_haversine, density_map, get_clusters, plot_embeddings\n",
    "from data import load_data\n",
    "from training import start_new_session, process_features, create_model\n",
    "#ATTENTION : PROBLMÈME À GERER : METHODE MERGE DE KERAS\n",
    "#la question se posera au moment de faire le modèle\n",
    "os.chdir('..')\n",
    "#inline display of plots :\n",
    "%matplotlib inline\n",
    "\n",
    "#random seed for reprodutibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coords = np.concatenate(data.train['POLYLINE_FULL'].as_matrix())\n",
    "clusters = get_clusters(all_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_new_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, input_dim=20, kernel_initializer=\"normal\")`\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.metadata, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Reshape, Activation, Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from utils import tf_haversine\n",
    "from data import load_data\n",
    "from utils import get_clusters\n",
    "\n",
    "\n",
    "def start_new_session():\n",
    "    \"\"\"\n",
    "    Starts a new Tensorflow session.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the session only uses the GPU memory that it actually needs\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    session = tf.Session(config=config, graph=tf.get_default_graph())\n",
    "    K.tensorflow_backend.set_session(session)\n",
    "\n",
    "\n",
    "def first_last_k(coords):\n",
    "    \"\"\"\n",
    "    Returns a list with the first k and last k GPS coordinates from the given trip.\n",
    "    The returned list contains 4k values (latitudes and longitudes for 2k points).\n",
    "    \"\"\"\n",
    "    k = 5\n",
    "    partial = [coords[0] for i in range(2*k)]\n",
    "    num_coords = len(coords)\n",
    "    if num_coords < 2*k:\n",
    "        partial[-num_coords:] = coords\n",
    "    else:\n",
    "        partial[:k] = coords[:k]\n",
    "        partial[-k:] = coords[-k:]\n",
    "    partial = np.row_stack(partial)\n",
    "    return np.array(partial).flatten()\n",
    "\n",
    "\n",
    "def process_features(df):\n",
    "    \"\"\"\n",
    "    Process the features required by our model from the given dataframe.\n",
    "    Return the features in a list so that they can be merged in our model's input layer.\n",
    "    \"\"\"\n",
    "    # Fetch the first and last GPS coordinates\n",
    "    coords = np.row_stack(df['POLYLINE'].apply(first_last_k))\n",
    "    # Standardize latitudes (odd columns) and longitudes (even columns)\n",
    "    latitudes = coords[:,::2]\n",
    "    coords[:,::2] = scale(latitudes)\n",
    "    longitudes = coords[:,1::2]\n",
    "    coords[:,1::2] = scale(longitudes)\n",
    "    \n",
    "    return [\n",
    "        df['QUARTER_HOUR'].as_matrix(),\n",
    "        df['DAY_OF_WEEK'].as_matrix(),\n",
    "        df['WEEK_OF_YEAR'].as_matrix(),\n",
    "        df['ORIGIN_CALL_ENCODED'].as_matrix(),\n",
    "        df['TAXI_ID_ENCODED'].as_matrix(),\n",
    "        df['ORIGIN_STAND_ENCODED'].as_matrix(),\n",
    "        coords,\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_model(metadata, clusters):\n",
    "    \"\"\"\n",
    "    Creates all the layers for our neural network model.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Arbitrary dimension for all embeddings\n",
    "    embedding_dim = 10\n",
    "\n",
    "    # Quarter hour of the day embedding\n",
    "    embed_quarter_hour = Sequential()\n",
    "    embed_quarter_hour.add(Embedding(metadata['n_quarter_hours'], embedding_dim, input_length=1))\n",
    "    embed_quarter_hour.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Day of the week embedding\n",
    "    embed_day_of_week = Sequential()\n",
    "    embed_day_of_week.add(Embedding(metadata['n_days_per_week'], embedding_dim, input_length=1))\n",
    "    embed_day_of_week.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Week of the year embedding\n",
    "    embed_week_of_year = Sequential()\n",
    "    embed_week_of_year.add(Embedding(metadata['n_weeks_per_year'], embedding_dim, input_length=1))\n",
    "    embed_week_of_year.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Client ID embedding\n",
    "    embed_client_ids = Sequential()\n",
    "    embed_client_ids.add(Embedding(metadata['n_client_ids'], embedding_dim, input_length=1))\n",
    "    embed_client_ids.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Taxi ID embedding\n",
    "    embed_taxi_ids = Sequential()\n",
    "    embed_taxi_ids.add(Embedding(metadata['n_taxi_ids'], embedding_dim, input_length=1))\n",
    "    embed_taxi_ids.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Taxi stand ID embedding\n",
    "    embed_stand_ids = Sequential()\n",
    "    embed_stand_ids.add(Embedding(metadata['n_stand_ids'], embedding_dim, input_length=1))\n",
    "    embed_stand_ids.add(Reshape((embedding_dim,)))\n",
    "    \n",
    "    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n",
    "    coords = Sequential()\n",
    "    coords.add(Dense(1, input_dim=20, init='normal'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Concatenate([\n",
    "                embed_quarter_hour,\n",
    "                embed_day_of_week,\n",
    "                embed_week_of_year,\n",
    "                embed_client_ids,\n",
    "                embed_taxi_ids,\n",
    "                embed_stand_ids,\n",
    "                coords\n",
    "            ]))\n",
    "\n",
    "    # Simple hidden layer\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Determine cluster probabilities using softmax\n",
    "    model.add(Dense(len(clusters)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Final activation layer: calculate the destination as the weighted mean of cluster coordinates\n",
    "    cast_clusters = K.cast_to_floatx(clusters)\n",
    "    def destination(probabilities):\n",
    "        return tf.matmul(probabilities, cast_clusters)\n",
    "    model.add(Activation(destination))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = SGD(lr=0.01, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n",
    "    model.compile(loss=tf_haversine, optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def full_train(n_epochs=100, batch_size=200, save_prefix=None):\n",
    "    \"\"\"\n",
    "    Runs the complete training process.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load initial data\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data()\n",
    "\n",
    "    # Estimate the GPS clusters\n",
    "    print(\"Estimating clusters...\")\n",
    "    clusters = get_clusters(data.train_labels)\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = []\n",
    "    if save_prefix is not None:\n",
    "        # Save the model's intermediary weights to disk after each epoch\n",
    "        file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n",
    "        callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    start_new_session()\n",
    "    model = create_model(data.metadata, clusters)\n",
    "    \n",
    "    # Run the training\n",
    "    print(\"Start training...\")\n",
    "    history = model.fit(\n",
    "        process_features(data.train), data.train_labels,\n",
    "        nb_epoch=n_epochs, batch_size=batch_size,\n",
    "        validation_data=(process_features(data.validation), data.validation_labels),\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    if save_prefix is not None:\n",
    "        # Save the training history to disk\n",
    "        file_path = 'cache/%s-history.pickle' % save_prefix\n",
    "        with open(file_path, 'wb') as handle:\n",
    "            pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Estimating clusters...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors > 0. Got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0e122f66d43c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfull_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-f02f43d0d1d8>\u001b[0m in \u001b[0;36mfull_train\u001b[1;34m(n_epochs, batch_size, save_prefix)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;31m# Estimate the GPS clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Estimating clusters...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_clusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;31m# Set up callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EA\\ECML-PKDD-2015\\code\\utils.py\u001b[0m in \u001b[0;36mget_clusters\u001b[1;34m(coords)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m# Further reduce the number of clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# (Note: the quantile parameter was tuned to find a significant and reasonable number of clusters)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mbandwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_bandwidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMeanShift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_seeding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\mean_shift_.py\u001b[0m in \u001b[0;36mestimate_bandwidth\u001b[1;34m(X, quantile, n_samples, random_state, n_jobs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     nbrs = NearestNeighbors(n_neighbors=int(X.shape[0] * quantile),\n\u001b[0;32m     72\u001b[0m                             n_jobs=n_jobs)\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mnbrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mbandwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'precomputed'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m         \"\"\"\n\u001b[1;32m--> 803\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    257\u001b[0m                 raise ValueError(\n\u001b[0;32m    258\u001b[0m                     \u001b[1;34m\"Expected n_neighbors > 0. Got %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m                 )\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors > 0. Got 0"
     ]
    }
   ],
   "source": [
    "full_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
